[TOC]

# What is Dropout ?

## 背景

Dropout是用于防止过拟合和提供一种有效近似联结指数级不同神经网络结构的方法。所谓过拟合便是模型在训练数据上表现优秀但是在测试集上表现很差的现象。由于随着神经网络的日益加深，网络参数越来越多，模型结构越来越复杂，在训练神经网络的时候非常容易出现过拟合现象。

## Dropout 原理

简单说来Dropout的原理便是**模型平均**，所谓模型平均，顾名思义，就是把来自不同模型的估计或者预测通过一定的权重平均起来。==Dropout中的drop指在每次训练神经网络是随机“丢弃”网络层中的某些节点==(所谓丢弃便是去掉节点间的边)，对一个网络使用dropout相当于从网络中采样一个“变薄”的网络，这个变薄的网络包含所有节点（不管是存活还是被丢弃）。因此，一个有$n$个节点的网络可以看作拥有$2^n$个“变薄”的网络的集合，这些网络共享权值，因此总的参数量还是$O(n^2)$或者更少。对于每一个训练样本，都有一个“薄网络”被采样训练，因此训练一个使用dropout的网络可以看成是在训练权值共享的$n^2$个“薄网络”的集合。

![](https://raw.githubusercontent.com/bovane/md_images/master/20190227212525.png)

dropout过程就是一个非常有效的神经网络模型平均方法，通过训练大量的不同的网络，来平均预测概率。不同的模型在不同的训练集上训练（每个批次的训练数据都是随机选择），最后在每个模型用相同的权重来“融合”，从而减低过拟合程度。

## Dropout训练和测试过程

1. 将输入$x$通过修改后的网络前向传播，得到的损失结果通过修改后的网络反向传播。一小批训练样本执行完该过程后按照随机梯度下降法更新（没有被删除的神经元）对应的参数$(w,b)$；
2. 重复下述过程直到网络收敛：
   - 恢复被“丢弃”的神经元（此时被“丢弃”的神经元的参数保持原样，而没有被“丢弃”的神经元的参数已经有所更新）
   - 以 $1-p$ 的概率临时“丢弃”（$p$的概率保留）网络中的隐层神经单元（备份被“丢弃”神经元的参数）
   - 对一小批训练样本，先前向传播然后反向传播损失并根据随机梯度下降法更新参数$(w,b)$，（没有被删除的那一部分参数得到更新，删除的神经元参数保持被删除前的结果）

在测试阶段，**显式地将训练中指数级的“薄网络”中求平均是不现实的**。实践中的思路是这样：在测试时使用一个不使用dropout的网络，该网络的权值是训练时的网络权值的缩小版，即，如果一个隐层单元在训练过程中以概率p被保留，那么该单元的输出权重在测试时乘以p（如下图所示）。这样共享权值的$2^n$个训练网络就可以在测试时近似联结成一个网络，因此能有效降低泛化误差。

![](https://raw.githubusercontent.com/bovane/md_images/master/20190227214223.png)

![](https://raw.githubusercontent.com/bovane/md_images/master/20190227214403.png)

![](https://raw.githubusercontent.com/bovane/md_images/master/20190227214540.png)



## Drop实现

```python
def dropout(x, keep_prob, noise_shape=None, seed=None, name=None)
# x -- training data
# keep_prob 保留概率
# tf.nn.dropout() 调用
# output：A Tensor of the same shape of x
# tensorflow中的dropout就是：使输入tensor中某些元素变为0，剩余非0的元素变为原来的1/keep_prob！
```

## 总结

