[TOC]

# Representation

​	自然语言处理(NLP)领域中最基本和最核心的概念就是：词表示(Representation)，和语音和图像等比较自然低级的数据结构相比而言，语言是一种高级抽象的数据结构，具有高度抽象的特征。在图像和语音领域，最基本的数据是信号数据，我们可以通过一些距离度量，判断信号是否相似，在判断两幅图片是否相似时，只需通过观察图片本身就能给出回答。但是对于文本语言来说，不能这样处理。**因为文本是符号数据，两个词只要字面不同，就难以刻画它们之间的联系，即使是“麦克风”和“话筒”这样的同义词，**从字面上也难以看出这两者意思相同==（语义鸿沟现象）==，可能并不是简单地一加一那么简单就能表示出来，而判断两个词是否相似时，还需要更多的背景知识才能做出回答。综上所述研究特定的词表示方法是非常有必要的。

## NLP词的表示类型

按照目前的发展情况，现在主流的词表示方法分为两种，第一种为独热one-hot表示，另外一种为分布式表示distributed。

### 词的one-hot表示

NLP 中最直观，也是到目前为止最常用的词表示方法是 One-hot Representation，这种方法把每个词表示为一个很长的向量。这个向量的维度是词表大小，其中绝大多数元素为 0，只有一个维度的值为 1，这个维度就代表了当前的词。**简而言之，one-hot就是在一个语料库中，给每个字/词编码一个索引，然后根据索引对词进行表示。**例如，我们用one-hot表示话筒和麦克两个词。

- “话筒”表示为 [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 ...]
- “麦克”表示为 [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ...]

**每个词都是茫茫 0 海中的一个 1**。这种 One-hot Representation 如果采用稀疏方式存储，会是非常的简洁：也就是给每个词分配一个数字 ID。如果要编程实现的话，用 Hash 表给每个词分配一个编号就可以了。这么简洁的表示方法配合上最大熵、SVM、CRF 等等算法已经很好地完成了 NLP 领域的各种主流任务。

#### one-hot优缺点分析

优点：简洁，存储代价小

缺点：词向量的维度会随着文本中词的数量类型增大而增大==(容易导致维灾难)==；**两个词之间是完全独立的，无法表示在语义层面上词语词之间的相关信息(致命缺点)。**

### 词的分布式表示

传统的独热表示（ one-hot representation）仅仅将词符号化，不包含任何语义信息。如何将语义融入到词表示中？Harris 在 1954 年提出的**分布假说（ distributional hypothesis）为这一设想提供了理论基础：上下文相似的词，其语义也相似。**Firth 在 1957 年对分布假说进行了进一步阐述和明确：词的语义由其上下文决定。

目前为止，基于分布假说的词表示方法，根据建模的不同，主要可以分为三类：

- 基于矩阵的分布表示
- 基于聚类的分布表示
- 基于神经网络的分布表示

尽管这些不同的分布表示方法使用了不同的技术手段获取词表示，但由于这些方法均基于分布假说，**它们的核心思想也都由两部分组成：**

- 选择一种方式描述上下文
- 选择一种模型刻画某个词（下文称“目标词”）与其上下文之间的关系

## NLP语言模型

目前为止NLP语言模型可以分为两类，第一类为文法模型(文法规则来源于语言学家掌握的语言学知识和领域知识，但这种语言模型不能处理大规模真实文本。)，第二类为统计模型,可用与计算机处理大规模文本。

### 统计语言模型

 **统计语言模型把语言（词的序列）看作一个随机事件，并赋予相应的概率来描述其属于某种语言集合的可能性。**给定一个词汇集合 V，对于一个由 V 中的词构成的序列$S = ⟨w_1, · · · , w_T ⟩ ∈ V_n$，统计语言模型赋予这个序列一个概率$P(S)$，来衡量S 符合自然语言的语法和语义规则的置信度。简而言之，统计语言模型就是计算一个句子概率大小的模型。

#### 常见的统计语言模型

- N-gram:考虑词形方面的特征，其又分为一元模型、二元模型、三元模型
- N-pos模型：考虑词类词性方面的特征，前一个词的词类决定下一个词出现的概率
- 基于决策树的语言模型、最大熵模型

[NLP语言模型综述](https://blog.csdn.net/lihaitao000/article/details/51159608)

## 词的分布式表示

### 基于矩阵的分布式表示

基于矩阵的分布表示通常又称为分布语义模型，在这种表示下，**矩阵中的一行，就成为了对应词的表示**，每列表示一种不同的上下文，矩阵中的每个元素对应相关词和上下文的共现次数。 这种表示描述了该词的上下文的分布。==由于分布假说认为上下文相似的词，其语义也相似，==因此在这种表示下，两个词的语义相似度可以直接转化为两个向量的空间距离。

该方法分为三个步骤： 
- 选取上下文。第一种：将词所在的文档作为上下文，形成“词-文档”矩阵。第二种：将词附近上下文中的各个词（如上下文窗口中的5个词）作为上下文，形成“词-词”矩阵。第三种：将词附近上下文各词组成的n-gram作为上下文，形成“词-n元词组”。 
- 确定矩阵中各元素的值。根据“词-上下文”共现矩阵的定义，里面各元素的值应为词与对应的上下文的共现次数。但一般采用多种加权和平滑方法，eg:TF-IDF

- 矩阵分解。常见分解技术：奇异值分解SVD、非负矩阵分解NMF、主成分分析PCA。

   常见到的Global Vector 模型（ GloVe模型）是一种对“词-词”矩阵进行分解从而得到词表示的方法，属于基于矩阵的分布表示。

### 基于聚类的分布表示

该方法以根据两个词的公共类别判断这两个词的语义相似度。最经典的方法是布朗聚类（Brown clustering）

### 基于神经网络的分布表示、word embedding

**基于神经网络的分布表示一般称为词向量、词嵌入（ word embedding）或分布式表示（ distributed representation）**

神经网络词向量表示技术**通过神经网络对上下文，以及上下文与目标词之间的关系进行建模**。由于神经网络较为灵活，这类方法的最大优势在于可以表示复杂的上下文。在前面基于矩阵的分布表示方法中，最常用的上下文是词。如果使用包含词序信息的 n-gram 作为上下文，当 $n$增加时， $n-gram$的总数会呈指数级增长，此时会遇到维数灾难问题。**而神经网络在表示 $n-gram$ 时，可以通过一些组合方式对 n 个词进行组合，参数个数仅以线性速度增长。**有了这一优势，神经网络模型可以对更复杂的上下文进行建模，在词向量中包含更丰富的语义信息。

## 词嵌入(word embedding)

基于神经网络的分布表示又称为词向量、词嵌入，神经网络词向量模型与其它分布表示方法一样，均基于分布假说，核心依然是上下文的表示以及上下文与目标词之间的关系的建模。

前面提到过，==为了选择一种模型刻画某个词（下文称“目标词”）与其上下文之间的关系，我们需要在词向量中capture到一个词的上下文信息。==同时，上面我们恰巧提到了统计语言模型正好具有捕捉上下文信息的能力。那么构建上下文与目标词之间的关系，最自然的一种思路就是使用语言模型。**从历史上看，早期的词向量只是神经网络语言模型的副产品。**  

2001年， Bengio 等人正式提出神经网络语言模型（ Neural Network Language Model ，NNLM），该模型在学习语言模型的同时，也得到了词向量。所以请注意一点：词向量可以认为是神经网络训练语言模型的副产品。

前面提过，one-hot表示法具有维度过大的缺点，那么现在将vector做一些改进：

- 将vector每一个元素由整形改为浮点型，变为整个实数范围的表示

- 将原来稀疏的巨大维度压缩嵌入到一个更小维度的空间。如图示：   

![](https://raw.githubusercontent.com/bovane/md_images/master/20190403202958.png)

[什么是word embedding?](https://zhuanlan.zhihu.com/p/27830489)

[理解word embedding](https://www.zhihu.com/question/32275069)

word embedding，**就是找到一个映射或者函数，生成在一个新的空间上的表达，**该表达就是word representation。

## 神经网络语言模型 and word2vec

通过神经网络训练语言模型可以得到词向量，那么，究竟有哪些类型的神经网络语言模型呢？常用的有下列语言模型：

-  Neural Network Language Model ，NNLM

-  Log-Bilinear Language Model， LBL
-  Recurrent Neural Network based Language Model，RNNLM
-  Collobert 和 Weston 在2008 年提出的 C&W 模型
-  Mikolov 等人提出了 CBOW（ Continuous Bagof-Words）和 Skip-gram 模型

上面提到的5个神经网络语言模型，**只是个在逻辑概念上的东西**，那么具体我们得通过设计将其实现出来，而实现CBOW（ Continuous Bagof-Words）和 Skip-gram 语言模型的工具正是word2vec。

[CBOW模型理解](https://blog.csdn.net/u010665216/article/details/78724856)

[Skip-gram模型理解](https://blog.csdn.net/u010665216/article/details/78721354)

统计语言模型statistical language model就是给你几个词，在这几个词出现的前提下来计算某个词出现的（事后）概率。**CBOW也是统计语言模型的一种，顾名思义就是根据某个词前面的C个词或者前后C个连续的词，来计算某个词出现的概率。**==Skip-Gram Model相反，是根据某个词，然后分别计算它前后出现某几个词的各个概率。==

Word embedding的训练方法大致可以分为两类：一类是无监督或弱监督的预训练；一类是端对端（end to end）的有监督训练。无监督或弱监督的预训练以word2vec和auto-encoder为代表。这一类模型的特点是，不需要大量的人工标记样本就可以得到质量还不错的embedding向量。不过因为缺少了任务导向，可能和我们要解决的问题还有一定的距离。因此，我们往往会在得到预训练的embedding向量后，用少量人工标注的样本去fine-tune整个模型。

 **相比之下，端对端的有监督模型在最近几年里越来越受到人们的关注**。与无监督模型相比，端对端的模型在结构上往往更加复杂。同时，也==因为有着明确的任务导向，端对端模型学习到的embedding向量也往往更加准确。==例如，通过一个embedding层和若干个卷积层连接而成的深度神经网络以实现对句子的情感分类，可以学习到语义更丰富的词向量表达。

[word2vec理解](https://www.jianshu.com/p/471d9bfbd72f)

[word2vec详解](https://zhuanlan.zhihu.com/p/53425736)

## 参考文献

[词表示的来龙去脉](https://blog.csdn.net/scotfield_msn/article/details/69075227)