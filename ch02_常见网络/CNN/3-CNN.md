[TOC]

# What's CNN?

卷积神经网络(CNN)是一类特殊的人工神经网络，区别于其他神经网(如递归神经网络等),其
最主要的特点是**卷积运算**操作。 因此，CNN在诸多领域应用特别是图像相关任务上表现优异，诸如，图像分类、图像语义检测等工作。此外，随着 CNN 研究的深入，如自然语言处理中的文本分类，软件工程数据挖掘中的软件缺陷预测等问题都在尝试利用卷积神经网络解决，**并取得了相比传统方法甚至其他深度网络模型更优的预测效果。**

## CNN发展历程

![](https://raw.githubusercontent.com/bovane/md_images/master/20190329193521.png)

### 开山之作:LeNet

卷积神经网络的开山之作LeCun在1998年提出，用于解决手写数字识别的视觉任务。自那时起，CNN的最基本的架构就定下来了：卷积层、池化层、全连接层。如今各大深度学习框架中所使用的LeNet都是简化改进过的LeNet-5（-5表示具有5个层），LeNet-5跟现有的$conv->pool->ReLU$的套路不同，它使用的方式是$conv1->pool->conv2->pool2$再接全连接层，**但是不变的是，卷积层后紧接池化层的模式依旧不变。**

![](https://raw.githubusercontent.com/bovane/md_images/master/20190303104156.png)

[LeNet网络结构详解](https://blog.csdn.net/Genius_zz/article/details/52804585)

### 王者归来：AlexNet

在2000年期间由于计算机算力以及各种因素限制，神经网络的研究陷入一个低估，直到AlexNet在2012年ImageNet竞赛中以超过第二名10.9个百分点的绝对优势一举夺冠，从此深度学习和卷积神经网络名声鹊起，深度学习的研究如雨后春笋般出现，AlexNet的出现可谓是卷积神经网络的王者归来。

AlexNet相比于之前的LeNet有几个闪光点：

- 更深的网络（其中包括五个卷积层、3个全连接层）
- 数据增广（通过随机裁剪图像实现，能避免过拟合）
- LRU（用ReLU代替原sigomd加快收敛并避免梯度饱和）
- Dropout（使神经元随机失活，减少参数，避免过拟合）
- LRN(貌似没什么用处)

![](https://raw.githubusercontent.com/bovane/md_images/master/20190303105140.png)

[AlexNet网络结构详解](https://blog.csdn.net/shenziheng1/article/details/70234043)

### 稳步前行：ZF-Net

ZFNet是2013ImageNet分类任务的冠军，其网络结构没什么改进，只是调了调参，性能较Alex提升了不少。

![](https://raw.githubusercontent.com/bovane/md_images/master/20190303111842.png)

### 越走越深：VGG-Nets

VGG-Nets是由牛津大学VGG（Visual Geometry Group）提出，是2014年ImageNet竞赛定位任务的第一名和分类任务的第二名的中的基础网络。为了解决初始化（权重初始化）等问题，VGG采用的是一种Pre-training的方式，这种方式在经典的神经网络中经常见得到，就是先训练一部分小网络，然后再确保这部分网络稳定之后，再在这基础上逐渐加深。

![](https://raw.githubusercontent.com/bovane/md_images/master/20190303111809.png)

[VGG网络详解](https://blog.csdn.net/jyy555555/article/details/80515562) 、[VGG](https://blog.csdn.net/dcrmg/article/details/79254654)

### 大浪推手：GoogLeNet

GoogLeNet跟AlexNet,VGG-Nets这种单纯依靠加深网络结构进而改进网络性能的思路不一样，它另辟幽径，在加深网络的同时（22层），也在网络结构上做了创新，**引入Inception结构代替了单纯的卷积+激活的传统操作**。GoogLeNet有以下闪光点：

- ==引入Inception结构==：基于保持神经网络结构的稀疏性，又能充分利用密集矩阵的高计算性能的出发点。Inception是一种网中网（Network In Network）的结构，即原来的结点也是一个网络。Inception一直在不断发展，目前已经V2、V3、V4了。Inception的结构如图所示，其中1*1卷积主要用来降维，用了Inception之后整个网络结构的宽度和深度都可扩大，能够带来2-3倍的性能提升。
- ==中间层的辅助LOSS单元==：GoogLeNet有3个LOSS单元，这样的网络设计是为了帮助网络的收敛。在中间层加入辅助计算的LOSS单元，目的是计算损失时让低层的特征也有很好的区分能力，从而让网络更好地被训练。
- ==后面的全连接层全部替换为简单的全局平均pooling==：

![](https://raw.githubusercontent.com/bovane/md_images/master/20190303114840.png)

[GoogLeNet网络详解](https://blog.csdn.net/shuzfan/article/details/50738394) 、[GoogLeNet补充](https://blog.csdn.net/dcrmg/article/details/79254654)

### 里程碑式创新：ResNet

ResNet在网络结构上做了大创新，而不再是简单的堆积层数，ResNet在卷积神经网络的新思路，绝对是深度学习发展历程上里程碑式的事件。ResNet闪光点如下：

- 层数非常深，已经超过百层
- 引入残差单元来解决退化问题

网络深度增加的一个问题在于**这些增加的层是参数更新的信号**，因为梯度是从后向前传播的，增加网络深度后，比较靠前的层梯度会很小。这意味着这些层基本上学习停滞了，**这就是梯度消失问题**。深度网络的第二个问题在于训练，当网络更深时意味着参数空间更大，优化问题变得更难，因此简单地去增加网络深度反而出现更高的训练误差，深层网络虽然收敛了，但网络却开始退化了，即增加网络层数却导致更大的误差，一个56层的网络的性能却不如20层的性能好，**这不是因为过拟合（训练集训练误差依然很高），这就是烦人的退化问题**。==残差网络ResNet设计一种残差模块让我们可以训练更深的网络。== 

[ResNet网络详解](https://www.jianshu.com/p/46d76bd56766) 、[残差网络详解](https://blog.csdn.net/u014665013/article/details/81985082)

### 继往开来：DenseNet

DenseNet（Dense Convolutional Network）主要还是和ResNet及Inception网络做对比，思想上有借鉴，但却是全新的结构，网络结构并不复杂，却非常有效，在CIFAR指标上全面超越ResNet。可以说DenseNet吸收了ResNet最精华的部分，并在此上做了更加创新的工作，使得网络性能进一步提升。==最主要的闪光点是提出密集连接的概念：缓解梯度消失问题，加强特征传播，鼓励特征复用，极大的减少了参数量==

![](https://raw.githubusercontent.com/bovane/md_images/master/20190303115110.png)

[DenseNet详解](https://zhuanlan.zhihu.com/p/43057737) 、[DenseNet论文解读](https://blog.csdn.net/u014380165/article/details/75142664)

## CNN前馈运算和反馈运算

### 前馈运算

总体来说，卷积神经网络是一种层次模型，其输入是原始数据，如 RGB图像、原始音频数据等。**卷积神经网络通过卷积操作、汇合操作和非线性激活函数映射等一系列操作的层层堆叠，将高层语义信息逐层由原始数据输入层中抽取出来，逐层抽象，这一过程便是“前馈运算”.** 

$x^1 \stackrel{w^1}{\longrightarrow}x^2 \stackrel{w^2}{\longrightarrow}x^3 \to ... x^{L-1} \stackrel{w^L}{\longrightarrow} x^L$

$z=L(w^L,y)$ 为最终的损失函数。同样以图像分类任务为例，假设网络已训练完毕，即其中参数 $ω_1,...,ω_L $ 已收敛到某最优解，此时可用此网络进行图像类别预测。预测过程实际就是一次网络的前馈运算：将测试集图像作为网络输入 $x_1$送进网络，之后经过第一层操作 $w_1$可得 $x_2$ ，依此下去⋯⋯直至输出$x_L∈ R_{C}$ 。在==利用交叉墒损失函==
==数训练后得到的网络中，$x_L $的每一维可表示 $x _1$分别隶属 C 个类别的后验概率==。如此，可通过下式得到输入图像 $x_{1}$对应的预测标记:$argmax  x_i^{L}$ .

### 反馈运算

批处理的随机梯度下降法在训练模型阶段随机选取 n 个样本作为一批（mini-batch）样本，**先通过前馈运算得到预测并计算其误差，**==后通过梯度下降法更新参数，梯度从后往前逐层反馈，直至更新到网络的第一层参数==，这样的一个参数更新过程称为一个“批处理过程”（mini-batch）。不同批处理之间按照无放回抽样遍历所有训练集样本，遍历一次训练样本称为“一轮”（epoch）。

假设某批处理前馈后得到 n 个样本上的误差为 z，且最后一层 L 为$ ℓ_2$ 损失函数，则易得：

$\frac{\partial z}{w^L}=0$ $\frac{\partial z}{x^L}=x^L -y$ 可以看出，实际上每层操作都对应了两部分导数：一部分是误差关于第 i 层参
数的导数$\frac{\part z}{w_i}$ ，另一部分是误差关于该层输入的导数$\frac{\part z}{x_i}$ 。每一层通过链式法则将参数不断更新到上一层。

![](https://raw.githubusercontent.com/bovane/md_images/master/20190303141744.png)

## 卷积神经网络基本构件

### 端到端的学习思想

“端到端”学习方式，整个学习流程并不进行人为的子问题划分，而是完全交给深度学习模型直接学习从原始输入到期望输出的映射。相比分治策略，“端到端”的学习方式具有协同增效的优势，有更大可能获得全局最优解。

![](https://raw.githubusercontent.com/bovane/md_images/master/20190303143251.png)

![](https://raw.githubusercontent.com/bovane/md_images/master/20190303143128.png)

### 卷积层

卷积层是卷积神经网络中的基础操作，甚至在网络最后起分类作用的全连接层在工程实现时也是由卷积操作替代的。卷积运算实际是分析数学中的一种运算方式，**在卷积神经网络中通常是仅涉及离散卷积的情形。**总的来说，==卷积的主要目的是为了从输入图像中提取特征。==卷积可以通过从输入的一小块数据中学到图像的特征，并可以保留像素间的空间关系。

**卷积的数学定义：**  

![](https://raw.githubusercontent.com/bovane/md_images/master/20190303234412.png)

**卷积实例：**![](https://raw.githubusercontent.com/bovane/md_images/master/20190304000921.png)



![](https://raw.githubusercontent.com/bovane/md_images/master/20190304155731.png)

假设在像素点$(x,y)$处可能存在物体边缘，则其四周的像素值应与 $(x,y)$ 处有显著差异。此时，如作用以整体
边缘滤波器 K ，可消除四周像素值差异小的图像区域而保留显著差异区域，以此可检测出物体边缘信息。检测颜色、形状、纹理等等众多基本模式（patten）的滤波器（卷积核）都可以包含在一个足够复杂的深层卷积神经网络中。**通过“组合” 这些滤波器（卷积核）以及随着网络后续操作的进行，基本而一般的模式会逐渐被抽象为具有高层语义的“概念”表示，**并以此对应到具体的样本类别。颇有“盲人摸象”后，将各自结果集大成之意。

[深刻理解卷积](https://www.zhihu.com/question/54677157/answer/141245297) [如何通俗理解卷积](https://www.zhihu.com/question/22298352)

[卷积骚操作汇总](https://zhuanlan.zhihu.com/p/29367273) 

### 汇合层(Pooling)

Pooling操作实际上是一种降采样操作。同卷积层操作不同，==汇合层不包含需要学习的参数==。使用时仅需指定汇合类型（平均值或最大值等）、汇合操作的核大小和汇合操作的步长等超参数等。![](https://raw.githubusercontent.com/bovane/md_images/master/20190304161519.png)



目前最常用的汇合方法为最大值汇合和平均值汇合，此外随机汇合也会使用。[池化方法总结](https://blog.csdn.net/danieljianfeng/article/details/42433475) 

在卷积神经网络过去的工作中，研究者普遍认为汇合层有如下三种功效：

-  特征不变性：汇合操作使模型更关注是否存在某些特征而不是特征具体的位置。可看作是一种很强的先验，使特征学习包含某种程度自由度，能容忍一些特征微小的位移。
- 特征降维。由于汇合操作的降采样作用，汇合结果中的一个元素对应于原输入数据的一个子区域，因此汇合相当于在空间范围内做了维度约减，从而使模型可以抽取更广范围的特征。同时减小了下一层输入大小，进而减小计算量和参数个数。
- 在一定程度防止过拟合，更方便优化。

但是Pooling并不是并不是卷积神经网络必须的元件或操作。研究者提出用==一种特殊的卷积操作==来代替汇合层实现降采样，进而构建一个只含卷积操作的网络，在某些分类精度上要优于传统的卷积汇合神经网络。

==简单总结卷积和池化==

**卷积:把数据通过一个卷积核变化成特征，便于后面的分离。计算方式与信号系统中的相同。**

**池化:把很多数据用最大值或者平均值代替。目的是降低数据量。**

### 激活函数

激活函数层又称非线性映射层，顾名思义，==激活函数的引入为的是增加整个网络的表达能力==（即非线性）。否则，若干线性操作层的堆叠仍然只能起到线性映射的作用，无法形成复杂的函数。

[常用激活函数比较](https://blog.csdn.net/JNingWei/article/details/79210904)

### 全连接层

全连接层在整个卷积神经网络中起到“分类器”的作用。如果说卷积层、汇合层和激活函数层等操作是将原始数据映射到隐层特征空间的话，全连接层则起到将学到的特征表示映射到样本的标记空间的作用。在实际使用中，全连接层可由卷积操作实现，对前层是全连接的全连接层可以转化为卷积核为 1 × 1 的卷积；而前层是卷积层的全连接层可以转化为卷积核为 h × w 的全局卷积，h 和 w 分别为前层卷积输出结果的高和宽。

[全连接层的作用](https://www.zhihu.com/question/41037974)

### 目标函数

全连接层是将网络特征映射到样本的标记空间做出预测，目标函数的作用则用来衡量该预测值与真实样本标记之间的误差。在当下的卷积神经网络中，**交叉熵损失函数和 ℓ 2 损失函数**分别是分类问题和回归问题中最为常用的目标函数。

[目标函数](https://blog.csdn.net/qq_28448117/article/details/79199835) [目标函数2](https://blog.csdn.net/u010099080/article/details/80574268)

## CNN 网络结构中的重要概念

上面介绍了卷积神经网络中几种基本部件：卷积、汇合、激活函数、全连接层和目标函数。虽说卷积神经网络模型就是这些基本部件的按序层叠，但是在实践中究竟如何“有机组合”才能让模型工作、发挥效能呢？本章主要介绍介绍卷积网络结构中的三个重要概念，以及对一个经典网络进行细致分析。

### 感受野

什么是感受野？感受野用来表示网络内部的不同位置的神经元对原图像的感受范围的大小。神经元之所以无法对原始图像的所有信息进行感知，是因为在这些网络结构中普遍使用卷积层和pooling层，在层与层之间均为局部相连（通过sliding filter）。**神经元感受野的值越大表示其能接触到的原始图像范围就越大，也意味着他可能蕴含更为全局、语义层次更高的特征；**而值越小则表示其所包含的特征越趋向于局部和细节。因此感受野的值可以大致用来判断每一层的抽象层次。

现代卷积神经网络拥有多层甚至超多层卷积操作，随着网络深度的加深，后层神经元在第一层输入层的感受野会随之增大。因此可以通过使用小卷积核+多层卷积达到与大卷积核相同的感受野。**此外采用小卷积核同时可带来其余两个优势：**第一，由于小卷积核需多层叠加，加深了网络深度进而增强了网络容量和复杂度；第二，增强网络容量的同时减少了参数个数。目前已有不少研究工作为提升模型预测能力通过改造现有卷积操作试图扩大原有卷积核在前层的感受野大小[如何理解空洞卷积网络](https://www.zhihu.com/question/54149221/answer/192025860)，或使原始感受野不再是矩形区域而是更自由可变的形状。

[感受野计算公式](https://zhuanlan.zhihu.com/p/28492837) [感受野理解](https://zhuanlan.zhihu.com/p/26663577)

###  分布式表示

神经网络中的“分布式表示”指“语义概念”到神经元是一个多对多映射，直观来讲，即每个语义概念由许多分布在不同神经元中被激活的模式表示；而每个神经元又可以参与到许多不同语义概念的表示中去。词包模型源自自然语
言处理领域，在计算机视觉中，**人们通常将图像局部特征作为一个视觉单词，将所有图像的局部特征作为词典,那么一张图像就可以用它的视觉单词来描述，**而这些视觉单词又可以通过词典的映射形成一条表示向量。很显然，这样的表示是离散式表示，其表示向量的每个维度可以对应一个明确的视觉模式或概念。![](https://raw.githubusercontent.com/bovane/md_images/master/20190304192407.png)

需指出的是，除了分布式表示特性，还可从图中发现神经网络响应的区域多呈现“稀疏”特性，即响应区域集中且占原图比例较小。[分布式的表示学习理解](https://juejin.im/entry/5afa5424f265da0ba3522ff1)

### 深度特征的层次性

卷积操作可以获取图像不同区域的特征而Pooling可以对这些特征进行融合和抽象。随着若干卷积和池化的堆叠，各层得到的深层特征逐渐从泛化特征(纹理、边缘)过渡到高层语义特征(躯干、脑袋)等。浅层卷积网络学习到的是基本模式，如纹理、边缘，随着网络的加深逐渐出现一些高层语义模式，如车轮、人脸等形状的模式。这就是深度特征的层次性，目前在深度学习领域，深度特征的层次性已成为共识。

### 经典网络结构分析

##  

## 参考文献

[卷积神经网络架构演变历史](https://www.cnblogs.com/skyfsm/p/8451834.html) 

[解析CNN魏秀参](http://product.dangdang.com/25576901.html) 













