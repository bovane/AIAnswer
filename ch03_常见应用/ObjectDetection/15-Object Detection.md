[TOC]

# Object Detection

目标检测任务从XX第一次被定义出后，便引起了研究人员的广泛兴趣。

## 1.Introduction

### 1.1什么是目标检测？

**目标检测任务被定义为一张图片中是否有给定类别(pre-defined)的实例，如果存在则返回实例的空间位置和实例在图中的范围**。如下所示：

![](https://raw.githubusercontent.com/bovane/md_images/master/20190508103149.png)

我们预先定义了${person、sheep、dog}$类别，输入一张图像，返回有给定类别的实例的空间位置,该图像中分别有5只sheep,一个person，一条dog，并且在每个实例周围我们划出了边界框，限定了该实例的范围。

​	同样的是，如果给定的是一段视频，那么目标检测便变为实时目标检测技术，这比静态的图片要求更高一些，更侧重实时性和准确性。

#### 目标检测效果的衡量指标

![](https://raw.githubusercontent.com/bovane/md_images/master/20190509234211.png)

P-R曲线围起来的面积就是AP值，通常来说一个越好的分类器，AP值越高

- mAP:即 Mean Average Precision即平均AP值，是对多个验证集个体求平均AP值，作为 object dection中衡量检测精度的指标。
- IOU: 即 Intersection-over-Union，表示了产生的**候选框（candidate bound）与原标记框（ground truth bound）的交叠率**或者说重叠度，也就是它们的交集与并集的比值。相关度越高该值。最理想情况是完全重叠，即比值为1
- ![](https://raw.githubusercontent.com/bovane/md_images/master/20190509234858.png)
- NMS:**即non maximum suppression即非极大抑制，**顾名思义就是抑制不是极大值的元素，搜索局部的极大值。在物体检测中，NMS的主要目的是为了清除多余的框，找到最佳的物体检测的位置。
- FPS：**每秒帧率（Frame Per Second，FPS），即每秒内可以处理的图片数量**。为了满足某些实时场景，如视频里面的监控等。

### 1.2目标检测有哪些应用场景

目标检测有非常多的应用场景，智慧物流快递分拣、视频监控分析、自动驾驶技术都需要用到目标检测技术。不同的应用场景对于目标检测有着不同的侧重。

### 1.3影响目标检测的因素

<font color=red>理想的检测器应当同时具有高准确率和高效率，但是由于真实世界太复杂，我们只有不断地逼近最理想地情况。</font>下面将分别从高准确率和高效率两个方面介绍，目标检测需要迎接的挑战。

![](https://raw.githubusercontent.com/bovane/md_images/master/20190510002235.png)

#### 高准确率

高准确率包括定位准确率和识别准确率，而由于真实世界复杂多变，我们需要我们的检测器足够健壮能够在各种真实情况下，达到较好的定位和识别效果。影响准确率的因素如下：

- **一个类别中有很多不同的实例，**比如它们只是不同的颜色、形状，比如车辆检测中，车这个类中包含很多不同的车，因此我们设计模型时，需要解决这个问题，而不是让模型把它们归于不同的类。
- **物体的多种多样，比如姿势、角度、形变不同**
- **图像所处的环境以及非结构化的背景，**比如光线、观察角度、尺寸、阴影、天气等因素
- **图像噪音**

我们也需要我们的检测器有超高的区分度

- **组内歧义（**如哈士奇和西伯利亚雪橇犬)
- **真实世界数以千计的类别**
- **结构化和无结构化的数据**

![](https://raw.githubusercontent.com/bovane/md_images/master/20190513170326.png)

#### 高效率

评价一个算法的优劣很大程度也取决于其执行效率，尤其是实时物体检测系统中，对于视频等进行物体检测，更是要求超高的效率，物体检测的效率可以分为时间开销、内存开销、存储开销。

- 真实世界中数以千计的物体类别
- 需要定位和识别物体
- 大量候选区域
- 大规模的图像/视频数据

真实世界中环境错综复杂，影响目标检测效果的因素有很多，上面所提到的影响准确率和效率的因素，都是从数据方面而言，==正是因为图片/视频存在各种各样的问题，研究人员针对问题提出各种各样的定义和模型来解决问题，==但是现存的一些的定义和模型并不能很好的解决一些问题，因此未来还需要提出更优秀的定义和模型。

数据的质量和特点很大程度上决定一个算法能否足够成功，而真实世界的目标检测环境非常复杂，==如果想要提高目标检测的性能，我们就需要找到对应的解决方法，将真实、多变的环境因素尽量消除，让模型能更好的表示和训练。==

## 2.目标检测相关技术

经过众多研究人员的不断发展，从第一次提出目标检测以来，现存的目标检测技术框架可以分为两类，第一类便是基于传统的手工提取特征进行目标检测，而传统的「目标检测」方法都是 `区域选择`、`提取特征`、`分类回归` 三部曲。

![](https://raw.githubusercontent.com/bovane/md_images/master/20190513172603.png)

**由于基于传统目标检测方法有两个难以解决的问题：**其一是区域选择的策略效果差、时间复杂度高；其二是手工提取的特征鲁棒性较差。但是自 2013 年一篇论文的发表，目标检测从原始的传统手工提取特征方法变成了基于卷积神经网络的特征提取，从此基于深度学习的目标检测成为主流。下面简要介绍基于传统和基于深度学习的目标检测方法。

### 2.1传统的解决方法

传统的目标检测解决方法大致可以分为以下三步：

- 候选区域选择：基于滑动窗口或者一些聚类的方法获取物体可能存在的区域
- 设计特征：设计一组特征作为分类器的输入，常见的特征如Haar特征、HOG特征、DPM特征等
- 选择分类器：SVM、决策树、基于弱分类器的Adaboost方法

下面介绍一些常见的组合算法：

- Haar特征+Adaboost+Cascade

- HOG特征+SVM对比HOG特征+Cascade
- DPM特征+Latent SVM

[传统目标检测方法](https://blog.csdn.net/qq_32742009/article/details/81388228)

### 2.2基于神经网络的解决方法

由于传统目标检测算法有两个通病，一是基于滑动窗口的区域选择策略没有针对性，时间复杂度高，窗口冗余，导致无法处理真实场景的物体检测需求，二是手工设计的特征对于多样性的变化没有很好的鲁棒性。于是研究者试图通过深度学习的方法解决传统目标检测算法的问题。

如今基于神经网络的目标检测方法有两种主流处理框架，一为有“两刀流”之称的R-CNN系列，另外一种是以YOLO系列代表的“一刀流”。

#### 两刀流

顾名思义，两刀解决问题：

1、生成可能区域（Region Proposal） & CNN 提取特征
2、放入分类器分类并修正位置

这一流派的算法都离不开 `Region Proposal` ，即是优点也是缺点，主要代表人物就是 `R-CNN`系。

**R-CNN -> SPP Net -> Fast R-CNN -> Faster R-CNN -> Mask R-CNN**

#### 一刀流

顾名思义，一刀解决问题，直接对**预测的目标物体进行回归**。
回归解决问题简单快速，但是太粗暴了，主要代表人物是 `YOLO` 和 `SSD` 。

------

无论 `两刀流` 还是 `一刀流`，他们都是在同一个天平下选取一个平衡点、或者选取一个极端—— 要么准，要么快。

两刀流的天平**主要倾向**准，
一刀流的天平**主要倾向**快。

但最后万剑归宗，大家也找到了自己的平衡，平衡点的有略微的不同。

## 3.[目标检测处理框架](https://github.com/bovane/AIAnswer/blob/master/18-RCNN-SSD.md)

### 3.1 R-CNN系的两刀流

### 3.2 YOLO 系的一刀流

## 4.未来研究方向

为了进一步提高提高目标检测的性能，未来的研究方向可能在下列领域发力。

### 4.1 特征表示

现有的方法都是基于卷积神经网络进行图像特征表示，

### 4.2 候选区域生成

### 4.3 上下文信息挖掘

### 4.4 训练策略

## 5.经典网络实现

### 5.1 R-CNN系列实现

### 5.2 YOLO系列实现

## 6.SUMMARY

### 6.1 ....

## 参考文献

- [机器之心-从R-CNN到RFBNet，目标检测架构5年演进全盘点](https://www.jiqizhixin.com/articles/092301)
- [深度学习小酒馆-基于深度学习的「目标检测」算法综述](https://zhuanlan.zhihu.com/p/33981103)
- [阿里云-一文读懂目标检测AI算法：R-CNN，faster R-CNN，yolo，SSD，yoloV2](https://yq.aliyun.com/articles/598428)